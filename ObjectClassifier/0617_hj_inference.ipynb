{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7c8e06",
   "metadata": {},
   "source": [
    "# Set Path & Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5c9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_0.001_Lamb_CosineAnnealing_example.pth' # 0.9032258064516129\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_0.001_adam_CosineAnnealing_example.pth' # 0.6129032258064516\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_1e-05_adam_CosineAnnealing_example.pth' # 0.8387096774193549\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_1e-05_Lamb_CosineAnnealing_example.pth' # 0.3870967741935484\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_1e-05_rmsprop_CosineAnnealing_example.pth' # 0.9354838709677419\n",
    "# model_path = '/home/lab16/jupyter_home/models/RegNet_1e-05_nadam_CosineAnnealing_example.pth' # 0.8709677419354839\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_0.001_adam_CosineAnnealing_example.pth'  # 0.3548387096774194\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_0.001_Lamb_CosineAnnealing_example.pth' # 0.7096774193548387\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_0.001_rmsprop_CosineAnnealing_example.pth' # 0.1935483870967742\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_1e-05_Lamb_CosineAnnealing_example.pth' # 0.12903225806451613\n",
    "# model_path = '/home/lab16/jupyter_home/models/ResNet50_1e-05_adam_CosineAnnealing_example.pth' # 0.6774193548387096\n",
    "# model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_0.001_adam_CosineAnnealing_example.pth' # 0.7096774193548387\n",
    "# model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_0.001_Lamb_CosineAnnealing_example.pth' # 0.6774193548387096\n",
    "# model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_1e-05_rmsprop_CosineAnnealing_example.pth' # 0.6129032258064516\n",
    "# model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_1e-05_nadam_CosineAnnealing_example.pth' # 0.5806451612903226\n",
    "# model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_1e-05_Lamb_CosineAnnealing_example.pth' # 0.12903225806451613\n",
    "model_path = '/home/lab16/jupyter_home/models/EfficientNetb4_1e-05_adam_CosineAnnealing_example.pth' # 0.5806451612903226\n",
    "\n",
    "#########################################################################################################\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/EfficientNetb4_0.0001_adam_CosineAnnealing_example.pth' # 0.6774193548387096\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/EfficientNetb4_0.0001_Lamb_CosineAnnealing_example.pth' # 0.41935483870967744\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/EfficientNetb4_0.0001_nadam_CosineAnnealing_example.pth' # 0.6774193548387096\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/EfficientNetb4_0.0001_rmsprop_CosineAnnealing_example.pth' # 0.6451612903225806\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/EfficientNetb4_0.001_nadam_CosineAnnealing_example.pth' # 0.6451612903225806\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/EfficientNetb4_0.001_rmsprop_CosineAnnealing_example.pth' # 0.6774193548387096\n",
    "\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/RegNet_0.0001_adam_CosineAnnealing_example.pth' # 0.8387096774193549\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/RegNet_0.0001_Lamb_CosineAnnealing_example.pth' # 0.6129032258064516\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/RegNet_0.0001_nadam_CosineAnnealing_example.pth' # 0.7096774193548387\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/RegNet_0.0001_rmsprop_CosineAnnealing_example.pth' # 0.6774193548387096\n",
    "\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/ResNet50_0.0001_adam_CosineAnnealing_example.pth' # 0.41935483870967744\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/ResNet50_0.0001_Lamb_CosineAnnealing_example.pth' # 0.6451612903225806\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/ResNet50_0.0001_nadam_CosineAnnealing_example.pth' # 0.6451612903225806\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/ResNet50_0.0001_rmsprop_CosineAnnealing_example.pth' # 0.6774193548387096\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/ResNet50_0.001_nadam_CosineAnnealing_example.pth' # 0.2903225806451613\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/ResNet50_1e-05_nadam_CosineAnnealing_example.pth' # 0.6129032258064516\n",
    "# model_path = '/home/lab16/jupyter_home/models/jy/ResNet50_1e-05_rmsprop_CosineAnnealing_example.pth' # 0.5483870967741935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d47ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/lab16/jupyter_home/test_img/test_all/' \n",
    "# path = '/home/lab16/jupyter_home/test_img/test_one/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d4d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_model = 'resnet'\n",
    "used_model = 'efficientnet'\n",
    "# used_model = 'regnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e63c13",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac329bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os.path import join as opj\n",
    "from glob import glob\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import albumentations as A\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c600b",
   "metadata": {},
   "source": [
    "# Dataset & Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddad2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        total_images_path = glob(path + '*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "            file_names.sort()\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.test_file_name = file_names\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size : {len(self.test_file_name)}')\n",
    "        print(self.test_file_name)\n",
    "\n",
    "    def __getitem__(self, idx): # test 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "        image = cv2.imread(opj(path, self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa36578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_augmentation(img_size):\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=(0.744859, 0.735139, 0.711357), std=(0.100712, 0.120692, 0.167998)),  \n",
    "                ])\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dd3ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset size : 31\n",
      "['25222_1.jpg' '25222_2.jpg' '25222_3.jpg' '25222_4.jpg' '25228_1.jpg'\n",
      " '35211_1.jpg' '35211_2.jpg' '35211_3.jpg' '35211_4.jpg' '35584_1.jpg'\n",
      " '35584_2.jpg' '35585_1.jpg' '45030_1.jpg' '45030_2.jpg' '45657_1.jpg'\n",
      " '45657_2.jpg' '45657_3.jpg' '45659_1.jpg' '45659_2.jpg' '45660_1.jpg'\n",
      " '45660_2.jpg' '45661_1.jpg' '45661_2.jpg' '55034_1.jpg' '55701_1.jpg'\n",
      " '55701_2.jpg' '55701_3.jpg' '55701_4.jpg' '55702_1.jpg' '55702_2.jpg'\n",
      " '55702_3.jpg']\n"
     ]
    }
   ],
   "source": [
    "test_transform = get_test_augmentation(img_size=256)\n",
    "test_dataset = Test_dataset(path, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=0, collate_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbc8726",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e9de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':256,\n",
    "    'EPOCHS':50,\n",
    "    'PATIENCE':10,\n",
    "    'class':14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacb5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(2048, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000,CFG['class'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # x = x.mean(dim=(-2, -1))\n",
    "        # (batch, 2048, 4, 4)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1263d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetb4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetb4, self).__init__()\n",
    "        model = models.efficientnet_b4(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(1792, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, CFG['class'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # (batch, 1792, 1, 1)\n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a09b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegNet, self).__init__()\n",
    "        model = models.regnet_y_16gf(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(3024, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, CFG['class'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # (batch, 3024, 1, 1)\n",
    "        \n",
    "#         x = torch.squeeze(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f010d3",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f9195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    if encoder_name == 'resnet':\n",
    "        model = ResNet50().to(device)\n",
    "    elif encoder_name == 'efficientnet':\n",
    "        model = EfficientNetb4().to(device)\n",
    "    elif encoder_name == 'regnet':\n",
    "        model = RegNet().to(device)\n",
    "        \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1b48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9664caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 11.09it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_arr = predict(used_model, test_loader, device, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab29a3fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.59501302e-01, 1.90069601e-01, 2.19948813e-02, 1.74433943e-02,\n",
       "        9.57810134e-03, 1.43203121e-02, 6.11179546e-02, 2.08042953e-02,\n",
       "        8.86416715e-03, 5.19255288e-02, 1.06468669e-03, 4.81028017e-03,\n",
       "        1.95567757e-01, 4.29377221e-02],\n",
       "       [4.91444618e-01, 4.82123762e-01, 6.59588957e-03, 1.03525631e-03,\n",
       "        4.51630389e-04, 1.19084795e-03, 2.01572990e-03, 2.49982555e-03,\n",
       "        3.51168674e-05, 2.53845588e-03, 7.16424711e-06, 9.92920468e-05,\n",
       "        7.44124223e-03, 2.52115750e-03],\n",
       "       [3.56031269e-01, 5.61798990e-01, 7.50592258e-03, 3.30690015e-03,\n",
       "        2.98941188e-04, 2.63825655e-02, 3.13244248e-03, 1.55781861e-03,\n",
       "        2.83089094e-03, 1.66381791e-03, 1.26314268e-03, 4.38673422e-03,\n",
       "        1.45607619e-02, 1.52798044e-02],\n",
       "       [2.01642290e-01, 6.15740478e-01, 3.67274648e-03, 6.09696023e-02,\n",
       "        6.97264681e-04, 1.26975095e-02, 4.69172327e-03, 8.21608678e-03,\n",
       "        2.44585937e-03, 2.03203894e-02, 1.11296296e-03, 9.60400421e-03,\n",
       "        5.11955917e-02, 6.99356152e-03],\n",
       "       [5.39956152e-01, 4.49898243e-01, 4.14827280e-03, 3.57121025e-04,\n",
       "        1.47761632e-04, 3.06570088e-03, 1.89007842e-04, 8.65283146e-05,\n",
       "        2.13981912e-05, 4.96243476e-04, 3.74109209e-06, 3.28485970e-04,\n",
       "        1.06564956e-03, 2.35770814e-04],\n",
       "       [4.98434249e-03, 4.30711021e-04, 9.67427552e-01, 2.72461853e-04,\n",
       "        1.07736960e-04, 2.23351084e-03, 2.76134291e-04, 1.61297663e-04,\n",
       "        1.54387765e-02, 5.79244050e-04, 1.19634636e-03, 2.97307386e-03,\n",
       "        6.74274925e-04, 3.24446056e-03],\n",
       "       [2.25233167e-01, 1.49132013e-02, 4.91797179e-01, 2.61668791e-03,\n",
       "        1.78450113e-03, 9.24628042e-03, 8.82713404e-03, 7.17656640e-03,\n",
       "        1.10539921e-01, 1.40304060e-03, 1.51142396e-03, 2.52083037e-02,\n",
       "        1.94239467e-02, 8.03187042e-02],\n",
       "       [1.58928037e-01, 2.74663083e-02, 7.63634324e-01, 1.52010994e-03,\n",
       "        1.21793209e-03, 2.12776531e-02, 2.70868861e-03, 1.65532401e-03,\n",
       "        1.98950362e-03, 1.27695996e-04, 4.91291634e-04, 1.38984881e-02,\n",
       "        2.81885033e-03, 2.26574019e-03],\n",
       "       [6.77638967e-03, 9.53663024e-04, 9.22194719e-01, 6.17448124e-04,\n",
       "        3.83225130e-03, 5.18721668e-03, 7.73071405e-03, 2.17403914e-03,\n",
       "        2.48366818e-02, 4.59790113e-04, 6.96369621e-04, 7.60649191e-03,\n",
       "        1.88322831e-03, 1.50510557e-02],\n",
       "       [1.74882587e-06, 1.66274076e-05, 4.75655652e-05, 9.98689711e-01,\n",
       "        5.27730735e-04, 7.63168064e-05, 1.16097817e-05, 3.26286536e-04,\n",
       "        6.30742898e-06, 4.98068475e-05, 1.00936959e-04, 1.93256346e-05,\n",
       "        1.12450936e-04, 1.36241770e-05],\n",
       "       [2.01568153e-04, 4.74170363e-03, 5.01143688e-04, 9.85171020e-01,\n",
       "        1.85304717e-03, 1.05688116e-03, 1.56421476e-04, 4.54902649e-03,\n",
       "        1.08237364e-05, 6.57911442e-05, 3.82270846e-05, 2.22713177e-04,\n",
       "        1.39011804e-03, 4.16847361e-05],\n",
       "       [1.32058340e-03, 5.75166021e-04, 1.93258896e-02, 6.95598796e-02,\n",
       "        8.83053303e-01, 1.95629662e-03, 3.79489432e-03, 1.56527839e-03,\n",
       "        2.49120430e-03, 1.07445321e-05, 3.91579015e-05, 9.76451091e-04,\n",
       "        2.40250118e-03, 1.29286479e-02],\n",
       "       [7.47865299e-04, 4.53341374e-04, 2.06749886e-03, 9.83765494e-05,\n",
       "        3.80256861e-05, 6.79559648e-01, 3.01060190e-05, 1.75987097e-05,\n",
       "        6.30271898e-05, 1.20961204e-05, 5.11530789e-06, 3.15550834e-01,\n",
       "        1.27308350e-03, 8.33465383e-05],\n",
       "       [8.24788549e-06, 2.75719867e-05, 1.13814767e-03, 5.58725078e-05,\n",
       "        1.80762199e-05, 9.87765789e-01, 2.48636439e-07, 5.99994678e-07,\n",
       "        1.15305750e-06, 2.18804672e-07, 2.21739256e-07, 1.09592453e-02,\n",
       "        2.40520840e-05, 5.36815548e-07],\n",
       "       [6.87728403e-03, 9.44183953e-03, 1.92789435e-02, 5.28964819e-03,\n",
       "        6.99746748e-03, 2.25131516e-04, 1.44488752e-01, 8.69306549e-02,\n",
       "        1.44406185e-02, 2.84183808e-02, 1.45451038e-03, 3.85343446e-05,\n",
       "        2.71417573e-02, 6.48976445e-01],\n",
       "       [9.60632926e-04, 2.35836254e-03, 3.74688860e-03, 4.39757202e-03,\n",
       "        2.42587877e-03, 2.61221605e-04, 1.21534817e-01, 5.20308256e-01,\n",
       "        7.68490892e-04, 4.11033966e-02, 2.34951192e-04, 3.34731194e-05,\n",
       "        6.62675276e-02, 2.35598534e-01],\n",
       "       [3.82425985e-03, 8.83649569e-04, 1.10187591e-03, 8.80137741e-05,\n",
       "        7.26242142e-04, 5.59465378e-04, 1.10011352e-02, 1.02373213e-02,\n",
       "        8.24580248e-03, 1.04412749e-01, 8.03648785e-04, 1.58666947e-03,\n",
       "        1.70345437e-02, 8.39494705e-01],\n",
       "       [1.19115546e-04, 2.46372765e-05, 6.77190498e-02, 3.89058515e-03,\n",
       "        3.28832283e-03, 1.02440768e-04, 9.95767768e-03, 3.40404711e-03,\n",
       "        5.33184826e-01, 1.33081409e-03, 3.28729004e-01, 1.62304495e-03,\n",
       "        5.68756484e-04, 4.60576117e-02],\n",
       "       [3.13765812e-03, 7.86522403e-04, 1.19240852e-02, 3.27578792e-03,\n",
       "        7.22818775e-04, 3.47627822e-04, 1.92161500e-02, 3.51166949e-02,\n",
       "        3.71039480e-01, 7.87355471e-03, 1.01743974e-01, 1.01430505e-03,\n",
       "        5.35559803e-02, 3.90245318e-01],\n",
       "       [2.15994442e-05, 2.02094197e-05, 2.48254015e-04, 8.00943992e-04,\n",
       "        1.89399929e-04, 3.67237590e-06, 6.19870871e-02, 1.32883281e-01,\n",
       "        1.28605396e-01, 4.49107707e-01, 1.79024488e-01, 1.23827631e-05,\n",
       "        1.47352531e-03, 4.56221774e-02],\n",
       "       [1.02918581e-04, 1.66348444e-04, 4.68560247e-05, 5.57020027e-03,\n",
       "        8.09362537e-05, 3.96249488e-05, 3.24613750e-02, 5.89420497e-01,\n",
       "        7.16016802e-04, 1.76207915e-01, 2.89968040e-04, 3.73502553e-05,\n",
       "        1.68896616e-01, 2.59633753e-02],\n",
       "       [8.13774765e-04, 4.27144259e-04, 2.22745668e-02, 1.00125661e-02,\n",
       "        4.71951952e-03, 7.69357372e-04, 1.17720803e-02, 4.85847034e-02,\n",
       "        1.74668804e-01, 3.08653116e-01, 4.13686410e-02, 5.88579976e-04,\n",
       "        6.88971579e-02, 3.06450039e-01],\n",
       "       [1.45454076e-03, 9.78548895e-04, 1.63001612e-01, 1.99829303e-02,\n",
       "        2.86673065e-02, 1.04292650e-02, 2.34661568e-02, 9.96149033e-02,\n",
       "        3.91565561e-01, 1.33746779e-02, 1.02261655e-01, 2.83670053e-02,\n",
       "        3.30126509e-02, 8.38231817e-02],\n",
       "       [1.83759650e-04, 2.26806878e-04, 1.24918137e-04, 7.67990350e-05,\n",
       "        4.06746040e-06, 7.82470345e-01, 1.72789369e-05, 1.10710389e-04,\n",
       "        6.29231226e-06, 1.61069675e-05, 7.35991080e-06, 2.15971366e-01,\n",
       "        7.66137149e-04, 1.79566814e-05],\n",
       "       [3.55524462e-05, 2.31583235e-05, 2.61538266e-03, 4.15674457e-03,\n",
       "        1.14479510e-03, 6.47939043e-04, 7.52703520e-03, 7.21738115e-02,\n",
       "        1.64942257e-03, 3.14618024e-04, 1.33109015e-05, 8.57551786e-05,\n",
       "        8.86023223e-01, 2.35892776e-02],\n",
       "       [2.77950836e-04, 3.40208731e-04, 1.66369649e-03, 5.62493429e-02,\n",
       "        2.77260412e-03, 5.70887023e-05, 3.19418013e-02, 1.02119640e-01,\n",
       "        3.84217082e-03, 2.06360817e-02, 1.36344141e-04, 1.17947129e-05,\n",
       "        3.87318373e-01, 3.92632961e-01],\n",
       "       [1.86927559e-06, 4.81647430e-06, 9.66215612e-06, 9.60235484e-04,\n",
       "        1.71733482e-05, 6.25646908e-06, 6.75010169e-03, 6.35231972e-01,\n",
       "        4.90002458e-05, 2.84121209e-03, 6.36369077e-06, 9.25872143e-07,\n",
       "        3.43958408e-01, 1.01618888e-02],\n",
       "       [3.78413279e-06, 2.36810638e-06, 5.08211360e-06, 1.85351731e-04,\n",
       "        4.95114773e-06, 1.43200987e-06, 1.40072708e-03, 4.38138843e-02,\n",
       "        4.35498914e-05, 3.45320324e-04, 9.53726840e-07, 3.91915194e-07,\n",
       "        9.40540552e-01, 1.36516485e-02],\n",
       "       [1.28547690e-04, 2.26412922e-05, 5.33992518e-03, 1.60282245e-04,\n",
       "        7.62380369e-04, 1.51583372e-04, 5.53869158e-02, 1.54461702e-02,\n",
       "        1.04917347e-01, 6.20336563e-04, 8.30875069e-04, 2.36395907e-04,\n",
       "        2.08609626e-02, 7.95135677e-01],\n",
       "       [1.21467911e-05, 1.48815730e-06, 1.00595855e-04, 3.41545419e-05,\n",
       "        5.90051532e-05, 1.01073306e-06, 3.37783135e-02, 5.67539549e-03,\n",
       "        6.62347861e-03, 1.07628061e-03, 1.55525559e-05, 1.59437320e-06,\n",
       "        3.36104408e-02, 9.19010460e-01],\n",
       "       [1.25757419e-04, 1.43636944e-05, 4.64417186e-04, 7.16454408e-04,\n",
       "        1.98429771e-04, 1.30387398e-05, 4.41634327e-01, 4.62302193e-02,\n",
       "        1.15095461e-02, 4.19588585e-04, 2.69867305e-04, 4.63446486e-05,\n",
       "        5.16417138e-02, 4.46715981e-01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e9a9f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  1,  1,  0,  2,  2,  2,  2,  3,  3,  4,  5,  5, 13,  7, 13,\n",
       "        8, 13,  9,  7,  9,  8,  5, 12, 13,  7, 12, 13, 13, 13])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_arr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "175b62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['25222_대만)망고케익184g', '25228_대만)파인애플케익184G', '35211_매일유업)데르뜨130G', '35584_매일데르뜨파인애플90G', '35585_매일데르뜨감귤90G', '45030_돌황도666G', '45657_씨제이)쁘티첼(요거젤리복숭아)', '45658_씨제이)쁘티첼(요거젤리밀감)', '45659_씨제이)쁘티첼(요거젤리딸기)', '45660_씨제이)쁘티첼(요거젤리화이트코코)', '45661_씨제이)쁘티첼(요거젤리블루베리)', '55034_돌트로피칼666G', '55701_쁘띠첼요거젤리밀감', '55702_쁘띠첼요거젤리복숭아']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754ee639",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {string : i for i, string in enumerate(labels)}\n",
    "label_decoder = {val:key for key, val in labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67cb31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25222_대만)망고케익184g\n",
      "25222_대만)망고케익184g\n",
      "25228_대만)파인애플케익184G\n",
      "25228_대만)파인애플케익184G\n",
      "25222_대만)망고케익184g\n",
      "35211_매일유업)데르뜨130G\n",
      "35211_매일유업)데르뜨130G\n",
      "35211_매일유업)데르뜨130G\n",
      "35211_매일유업)데르뜨130G\n",
      "35584_매일데르뜨파인애플90G\n",
      "35584_매일데르뜨파인애플90G\n",
      "35585_매일데르뜨감귤90G\n",
      "45030_돌황도666G\n",
      "45030_돌황도666G\n",
      "55702_쁘띠첼요거젤리복숭아\n",
      "45658_씨제이)쁘티첼(요거젤리밀감)\n",
      "55702_쁘띠첼요거젤리복숭아\n",
      "45659_씨제이)쁘티첼(요거젤리딸기)\n",
      "55702_쁘띠첼요거젤리복숭아\n",
      "45660_씨제이)쁘티첼(요거젤리화이트코코)\n",
      "45658_씨제이)쁘티첼(요거젤리밀감)\n",
      "45660_씨제이)쁘티첼(요거젤리화이트코코)\n",
      "45659_씨제이)쁘티첼(요거젤리딸기)\n",
      "45030_돌황도666G\n",
      "55701_쁘띠첼요거젤리밀감\n",
      "55702_쁘띠첼요거젤리복숭아\n",
      "45658_씨제이)쁘티첼(요거젤리밀감)\n",
      "55701_쁘띠첼요거젤리밀감\n",
      "55702_쁘띠첼요거젤리복숭아\n",
      "55702_쁘띠첼요거젤리복숭아\n",
      "55702_쁘띠첼요거젤리복숭아\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "\n",
    "for i in range(len(test_dataset.test_file_name)):\n",
    "    prediction = label_decoder[predict_arr.argmax(axis=1)[i]]\n",
    "    print(prediction)\n",
    "    pred_list.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44e323b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471c185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "439a5567",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39c1ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cbf963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images_path = glob(path + '*.jpg')\n",
    "file_names = []\n",
    "for i in range(len(total_images_path)):\n",
    "    file_names.append(os.path.basename(total_images_path[i])[:5])\n",
    "    file_names.sort()\n",
    "# file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bd7e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred_list = []\n",
    "for i in range(len(total_images_path)):\n",
    "    _pred_list.append(pred_list[i][:5])\n",
    "# _pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c381ef6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5806451612903226"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(_pred_list, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16868207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "535c5d41",
   "metadata": {},
   "source": [
    "with validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3ced8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# import torch.nn.functional as F\n",
    "# import albumentations as A\n",
    "# import albumentations.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d060fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_valid_data(data_dir):\n",
    "#     img_valid_list = []\n",
    "#     label_valid_list = []\n",
    "    \n",
    "#     image_path = os.path.join(data_dir, 'dessert')\n",
    "    \n",
    "#     for product_name in os.listdir(image_path):\n",
    "#         product_path = os.path.join(image_path, product_name)\n",
    "#         if os.path.isdir(product_path):\n",
    "#             # get image path\n",
    "#             img_valid_list.extend(glob(os.path.join(product_path, '*.jpg')))\n",
    "#             img_valid_list.extend(glob(os.path.join(product_path, '*.png')))\n",
    "#             label = list(product_name[:5])\n",
    "            \n",
    "#             # get label\n",
    "#             label_valid_list.append(''.join(label))\n",
    "                \n",
    "#     return img_valid_list, label_valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45d3d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def valid_data_blanced(img, label):\n",
    "#     x = []\n",
    "#     y = []\n",
    "    \n",
    "#     for i in range(CFG['class']):\n",
    "#         _img = img[(i * 15): ((i + 1) * 15)]\n",
    "#         _label = label[i]\n",
    "        \n",
    "#         for img_product in _img:\n",
    "#             x.append(img_product)\n",
    "#             y.append(_label)\n",
    "            \n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "973a8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_valid_list, label_valid_list = get_valid_data('./Data/product_image/Validation/')\n",
    "# x_valid, y_valid = valid_data_blanced(img_valid_list, label_valid_list)\n",
    "# len(label_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b00b0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# le2 = preprocessing.LabelEncoder()\n",
    "# targets_y = le2.fit_transform(y_valid)\n",
    "# targets_y = torch.as_tensor(targets_y)\n",
    "# one_hot_valid_y = F.one_hot(targets_y)\n",
    "# one_hot_valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e85ca4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AlbumentationsCustomDataset(Dataset):\n",
    "#     def __init__(self, img_path_list, label_list, train_mode=True, transforms=None):\n",
    "#         self.transforms = transforms\n",
    "#         self.train_mode = train_mode\n",
    "#         self.img_path_list = img_path_list\n",
    "#         self.label_list = label_list\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         img_path = self.img_path_list[index]\n",
    "#         # Get image data\n",
    "#         image = cv2.imread(img_path)\n",
    "        \n",
    "#         # By default OpenCV uses BGR color space for color images,\n",
    "#         # so we need to convert the image to RGB color space.\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         if self.train_mode:\n",
    "# #             image = image.astype(np.int16)\n",
    "#             augmented = self.transforms(image=image)\n",
    "#             image = augmented['image']\n",
    "#             label = self.label_list[index]\n",
    "#             return image, label\n",
    "#         else:\n",
    "#             image = self.transforms(image)\n",
    "#             label = self.label_list[index]\n",
    "#             return image, label\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8305a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_test_transform = albumentations.Compose([\n",
    "#                                     A.Resize(256, 256),\n",
    "#                                     A.Normalize(mean=(0.744859, 0.735139, 0.711357), std=(0.100712, 0.120692, 0.167998)),  \n",
    "# #                                     A.pytorch.transforms.ToTensor(),\n",
    "#                                     A.pytorch.transforms.ToTensorV2(transpose_mask=True),\n",
    "#                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccfea251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_vali_dataset = AlbumentationsCustomDataset(x_valid, one_hot_valid_y, train_mode=True, transforms=A_test_transform)\n",
    "# A_vali_loader = DataLoader(A_vali_dataset, batch_size = 5, shuffle=False, num_workers=0, collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85729923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_for_acc(model, test_loader, device):\n",
    "#     model.eval()\n",
    "#     model_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for img, label in tqdm(iter(test_loader)):\n",
    "#             img = img.float().to(device)\n",
    "            \n",
    "#             pred_logit = model(img)\n",
    "#             pred_logit = pred_logit.squeeze().detach().cpu()\n",
    "            \n",
    "#             model_pred.extend(pred_logit.tolist())\n",
    "#     return model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76b9b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7336bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if used_model == 'resnet':\n",
    "#     model = ResNet50().to(device)\n",
    "# elif used_model == 'efficientnet':\n",
    "#     model = EfficientNetb4().to(device)\n",
    "# elif used_model == 'regnet':\n",
    "#     model = RegNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8932b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "# preds = predict_for_acc(model, A_vali_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fdb7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels = np.argmax(preds, axis=1)\n",
    "# true_labels = one_hot_valid_y.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f0b6e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# accuracy_score(true_labels, pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_16] *",
   "language": "python",
   "name": "conda-env-pytorch_16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
