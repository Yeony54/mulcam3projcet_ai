{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c38e7b2",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81117f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  3 20:16:45 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   32C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c90d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging  # 로그 출력\n",
    "import easydict  # 속성으로 dict 값에 access할 수 있음\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # process bar\n",
    "from os.path import join as opj\n",
    "from ptflops import get_model_complexity_info\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c90e8a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff47ae",
   "metadata": {},
   "source": [
    "Hyper-parameter 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d4e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     # /home/lab18/Data/product_image/Training/image/10268_아넬라사과디저트_2입/\n",
    "     'data_path':'/home/lab16/jupyter_home/Data/product_image/',\n",
    "     'Kfold':1,\n",
    "     'model_path':'results/',\n",
    "\n",
    "     # Model parameter settings \n",
    "     'encoder_name':'regnety_064',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':224,\n",
    "     'batch_size':16,\n",
    "     'epochs':50,\n",
    "     'optimizer':'Lamb',\n",
    "#      'optimizer':'Adadelta',\n",
    "     'initial_lr':5e-6,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':5,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-6,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience':50,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':False,\n",
    "     'logging':False,\n",
    "     'num_workers':0,\n",
    "     'seed':42\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf0e95",
   "metadata": {},
   "source": [
    "# Utils for training and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d74a4b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimizer(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f420341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34eb9c5b",
   "metadata": {},
   "source": [
    "# Dataset & Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e8c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.datasets import ImageFolder\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c29f8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f22fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bdf031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ae0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 레이블을 one-hot-vector로 변환\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# targets = le.fit_transform(y)\n",
    "# targets = torch.as_tensor(targets)\n",
    "# one_hot_y = F.one_hot(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f1892dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation용 추가\n",
    "class Valid_Dataset(Dataset):\n",
    "    def __init__(self, transform=None): \n",
    "        total_images_path = glob('/home/lab16/jupyter_home/Data/product_image/Validation/image/**/*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.file_name = file_names\n",
    "        \n",
    "        each_label = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            each_label.append(os.path.basename(total_images_path[i])[:5])\n",
    "            \n",
    "        # 레이블을 one-hot-vector로 변환\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        targets = le.fit_transform(each_label)\n",
    "        targets = torch.as_tensor(targets)\n",
    "#         one_hot_y = F.one_hot(targets)\n",
    "            \n",
    "        self.target = np.array(targets) # 목표는 label\n",
    "#         self.target = one_hot_y\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Validation Dataset size:{len(self.file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx):  # train 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/train_256_new/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Validation/total_image/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        target = self.target[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "        # HWC => CHW-layout 변환\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ad6970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "#     def __init__(self, df, transform=None):\n",
    "    def __init__(self, transform=None):\n",
    "#         self.file_name = df['file_name'].values      \n",
    "        \n",
    "        total_images_path = glob('/home/lab16/jupyter_home/Data/product_image/Training/image/**/*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.file_name = file_names\n",
    "                \n",
    "        each_label = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            each_label.append(os.path.basename(total_images_path[i])[:5])\n",
    "\n",
    "        # 레이블을 one-hot-vector로 변환\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        targets = le.fit_transform(each_label)\n",
    "        targets = torch.as_tensor(targets)\n",
    "#         one_hot_y = F.one_hot(targets)\n",
    "            \n",
    "        self.target = np.array(targets) # 목표는 label\n",
    "#         self.target = one_hot_y\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Training Dataset size:{len(self.file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx):  # train 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/train_256_new/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Training/total_image/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        target = self.target[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "        # HWC => CHW-layout 변환\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.test_file_name = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.test_file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx): # test 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/test_256_new/', self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Test/total_image/', self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_file_name)\n",
    "\n",
    "# def get_loader(df, phase: str, batch_size, shuffle, num_workers, transform):\n",
    "def get_loader(phase: str, batch_size, shuffle, num_workers, transform):\n",
    "    if phase == 'test':\n",
    "#         dataset = Test_dataset(df, transform)  \n",
    "        dataset = Test_dataset(transform) \n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "    # 임시로 추가\n",
    "    elif phase == 'validation':\n",
    "#         dataset = Train_Dataset(df, transform)\n",
    "        dataset = Valid_Dataset(transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "    else:\n",
    "#         dataset = Train_Dataset(df, transform)\n",
    "        path = ''\n",
    "        dataset = Train_Dataset(transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "        \n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver == 1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "#                 transforms.ToTensor()\n",
    "                ])\n",
    "\n",
    "    if ver == 2:\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomPerspective(),\n",
    "                transforms.RandomAffine((20)),  # x, y축으로 이미지 늘림\n",
    "                transforms.RandomRotation(90),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "#                 transforms.ToTensor()\n",
    "            ])\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ce401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "496a3605",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c73038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # 사전 학습된 모델 사용하기\n",
    "        self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=args.drop_path_rate,\n",
    "                                    )\n",
    "        \n",
    "        if 'regnet' in args.encoder_name:        \n",
    "            num_head = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Linear(num_head, 76)\n",
    "        \n",
    "        elif 'efficient' in args.encoder_name:\n",
    "            num_head = self.encoder.classifier.in_features\n",
    "            self.encoder.classifier = nn.Linear(num_head, 76)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=0,\n",
    "                                    )\n",
    "        \n",
    "        if 'regnet' in encoder_name:        \n",
    "            num_head = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Linear(num_head, 76)\n",
    "        \n",
    "        elif 'efficient' in encoder_name:\n",
    "            num_head = self.encoder.classifier.in_features\n",
    "            self.encoder.classifier = nn.Linear(num_head, 76)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574592ec",
   "metadata": {},
   "source": [
    "# Trainer for Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3456802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f'device:{self.device}')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log_0603.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "#         df_train = pd.read_csv(opj(args.data_path, 'train_df.csv'))\n",
    "#         print('Read train_df.csv')\n",
    "\n",
    "#         kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "#         for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['label'])):\n",
    "#             df_train.loc[val_idx, 'fold'] = fold\n",
    "#         val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "#         df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "#         df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "        \n",
    "        # 수정 - TrainLoader\n",
    "        self.train_loader = get_loader(phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(phase='validation', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        \n",
    "        # TrainLoader\n",
    "#         self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "#                                        num_workers=args.num_workers, transform=self.train_transform)\n",
    "#         self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "#                                        num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "        macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
    "                                                 print_per_layer_stat=False, verbose=False)\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "#         self.optimizer = optim.Adadelta(self.model.parameters())\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "                \n",
    "            print(f'\\nbest epoch:{best_epoch}/loss:{best_loss:.4f}/f1:{best_f1:.4f}')\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66f503",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d2a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5763bf",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0beb201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_064', \n",
    "                             test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc682d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold X\n",
    "def result(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_064', \n",
    "                             test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = predict_list[0]\n",
    "\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8c70a",
   "metadata": {},
   "source": [
    "# Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93d602b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866d58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('./data/sample_submission.csv')\n",
    "# df_train = pd.read_csv('./data/train_df.csv')\n",
    "# df_test = pd.read_csv('./data/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cae8472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "# test_dataset = Test_dataset(df_test, test_transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe785ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e377aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77ed23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 20:16:47,354 INFO: {'exp_num': '0', 'data_path': '/home/lab16/jupyter_home/Data/product_image/', 'Kfold': 1, 'model_path': 'results/', 'encoder_name': 'regnety_064', 'drop_path_rate': 0.2, 'img_size': 224, 'batch_size': 16, 'epochs': 50, 'optimizer': 'Lamb', 'initial_lr': 5e-06, 'weight_decay': 0.001, 'aug_ver': 2, 'scheduler': 'cycle', 'warm_epoch': 5, 'max_lr': 0.001, 'min_lr': 5e-06, 'tmax': 145, 'patience': 50, 'clipping': None, 'amp': True, 'multi_gpu': False, 'logging': False, 'num_workers': 0, 'seed': 42, 'fold': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<---- Training Params ---->\n",
      "device:cuda\n",
      "Training Dataset size:8664\n",
      "Validation Dataset size:1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 20:16:47,869 INFO: Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_064-0a48325c.pth)\n",
      "2022-06-03 20:16:51,533 INFO: Computational complexity:       6.37 GMac\n",
      "2022-06-03 20:16:51,534 INFO: Number of parameters:           29.38 M \n",
      "100%|█████████████████████████████████████████| 542/542 [33:42<00:00,  3.73s/it]\n",
      "2022-06-03 20:50:34,301 INFO: Epoch:[001/050]\n",
      "2022-06-03 20:50:34,302 INFO: Train Loss:4.407 | Acc:0.0129 | F1:0.0068\n",
      "2022-06-03 20:54:46,610 INFO: val Loss:4.411 | Acc:0.0140 | F1:0.0062\n",
      "2022-06-03 20:54:47,292 INFO: -----------------SAVE:1epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:1/loss:4.4110/f1:0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [33:35<00:00,  3.72s/it]\n",
      "2022-06-03 21:28:22,541 INFO: Epoch:[002/050]\n",
      "2022-06-03 21:28:22,542 INFO: Train Loss:4.410 | Acc:0.0128 | F1:0.0055\n",
      "2022-06-03 21:32:28,346 INFO: val Loss:4.414 | Acc:0.0123 | F1:0.0064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:1/loss:4.4110/f1:0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [33:25<00:00,  3.70s/it]\n",
      "2022-06-03 22:05:54,074 INFO: Epoch:[003/050]\n",
      "2022-06-03 22:05:54,074 INFO: Train Loss:4.404 | Acc:0.0115 | F1:0.0065\n",
      "2022-06-03 22:10:03,436 INFO: val Loss:4.418 | Acc:0.0105 | F1:0.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:1/loss:4.4110/f1:0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [33:37<00:00,  3.72s/it]\n",
      "2022-06-03 22:43:40,621 INFO: Epoch:[004/050]\n",
      "2022-06-03 22:43:40,621 INFO: Train Loss:4.400 | Acc:0.0126 | F1:0.0064\n",
      "2022-06-03 22:47:51,396 INFO: val Loss:4.423 | Acc:0.0114 | F1:0.0029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:1/loss:4.4110/f1:0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [33:23<00:00,  3.70s/it]\n",
      "2022-06-03 23:21:14,958 INFO: Epoch:[005/050]\n",
      "2022-06-03 23:21:14,958 INFO: Train Loss:4.392 | Acc:0.0143 | F1:0.0086\n",
      "2022-06-03 23:25:20,022 INFO: val Loss:4.405 | Acc:0.0175 | F1:0.0089\n",
      "2022-06-03 23:25:21,066 INFO: -----------------SAVE:5epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:5/loss:4.4047/f1:0.0089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [33:17<00:00,  3.68s/it]\n",
      "2022-06-03 23:58:38,336 INFO: Epoch:[006/050]\n",
      "2022-06-03 23:58:38,337 INFO: Train Loss:4.369 | Acc:0.0136 | F1:0.0088\n",
      "2022-06-04 00:02:44,130 INFO: val Loss:4.335 | Acc:0.0158 | F1:0.0132\n",
      "2022-06-04 00:02:45,193 INFO: -----------------SAVE:6epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:6/loss:4.3349/f1:0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [37:21<00:00,  4.14s/it]\n",
      "2022-06-04 00:40:06,733 INFO: Epoch:[007/050]\n",
      "2022-06-04 00:40:06,734 INFO: Train Loss:4.328 | Acc:0.0180 | F1:0.0152\n",
      "2022-06-04 00:46:10,579 INFO: val Loss:4.275 | Acc:0.0289 | F1:0.0219\n",
      "2022-06-04 00:46:11,979 INFO: -----------------SAVE:7epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:7/loss:4.2745/f1:0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [41:26<00:00,  4.59s/it]\n",
      "2022-06-04 01:27:38,238 INFO: Epoch:[008/050]\n",
      "2022-06-04 01:27:38,239 INFO: Train Loss:4.291 | Acc:0.0242 | F1:0.0220\n",
      "2022-06-04 01:31:46,849 INFO: val Loss:4.188 | Acc:0.0456 | F1:0.0374\n",
      "2022-06-04 01:31:47,873 INFO: -----------------SAVE:8epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:8/loss:4.1876/f1:0.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [34:41<00:00,  3.84s/it]\n",
      "2022-06-04 02:06:29,540 INFO: Epoch:[009/050]\n",
      "2022-06-04 02:06:29,541 INFO: Train Loss:4.198 | Acc:0.0429 | F1:0.0378\n",
      "2022-06-04 02:12:29,834 INFO: val Loss:3.983 | Acc:0.0825 | F1:0.0677\n",
      "2022-06-04 02:12:44,364 INFO: -----------------SAVE:9epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:9/loss:3.9828/f1:0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:28<00:00,  5.37s/it]\n",
      "2022-06-04 03:01:12,725 INFO: Epoch:[010/050]\n",
      "2022-06-04 03:01:12,726 INFO: Train Loss:4.017 | Acc:0.0672 | F1:0.0577\n",
      "2022-06-04 03:07:14,616 INFO: val Loss:3.775 | Acc:0.1202 | F1:0.1011\n",
      "2022-06-04 03:07:19,882 INFO: -----------------SAVE:10epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:10/loss:3.7753/f1:0.1011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:19<00:00,  5.35s/it]\n",
      "2022-06-04 03:55:39,112 INFO: Epoch:[011/050]\n",
      "2022-06-04 03:55:39,114 INFO: Train Loss:3.855 | Acc:0.0873 | F1:0.0787\n",
      "2022-06-04 04:01:44,123 INFO: val Loss:3.514 | Acc:0.1430 | F1:0.1315\n",
      "2022-06-04 04:01:45,631 INFO: -----------------SAVE:11epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:11/loss:3.5138/f1:0.1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:29<00:00,  5.37s/it]\n",
      "2022-06-04 04:50:15,250 INFO: Epoch:[012/050]\n",
      "2022-06-04 04:50:15,252 INFO: Train Loss:3.715 | Acc:0.1078 | F1:0.0996\n",
      "2022-06-04 04:56:23,104 INFO: val Loss:3.329 | Acc:0.1711 | F1:0.1600\n",
      "2022-06-04 04:56:24,518 INFO: -----------------SAVE:12epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:12/loss:3.3286/f1:0.1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:52<00:00,  5.41s/it]\n",
      "2022-06-04 05:45:17,060 INFO: Epoch:[013/050]\n",
      "2022-06-04 05:45:17,062 INFO: Train Loss:3.558 | Acc:0.1371 | F1:0.1298\n",
      "2022-06-04 05:51:24,376 INFO: val Loss:3.259 | Acc:0.2044 | F1:0.1911\n",
      "2022-06-04 05:51:25,548 INFO: -----------------SAVE:13epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:13/loss:3.2590/f1:0.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:55<00:00,  5.42s/it]\n",
      "2022-06-04 06:40:20,624 INFO: Epoch:[014/050]\n",
      "2022-06-04 06:40:20,625 INFO: Train Loss:3.445 | Acc:0.1567 | F1:0.1491\n",
      "2022-06-04 06:46:29,646 INFO: val Loss:3.022 | Acc:0.2254 | F1:0.2161\n",
      "2022-06-04 06:46:34,848 INFO: -----------------SAVE:14epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:14/loss:3.0222/f1:0.2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:28<00:00,  5.37s/it]\n",
      "2022-06-04 07:35:03,445 INFO: Epoch:[015/050]\n",
      "2022-06-04 07:35:03,447 INFO: Train Loss:3.342 | Acc:0.1721 | F1:0.1663\n",
      "2022-06-04 07:41:10,064 INFO: val Loss:3.194 | Acc:0.2114 | F1:0.2119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:14/loss:3.0222/f1:0.2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:15<00:00,  5.34s/it]\n",
      "2022-06-04 08:29:25,978 INFO: Epoch:[016/050]\n",
      "2022-06-04 08:29:25,979 INFO: Train Loss:3.259 | Acc:0.1906 | F1:0.1854\n",
      "2022-06-04 08:35:29,926 INFO: val Loss:2.966 | Acc:0.2325 | F1:0.2250\n",
      "2022-06-04 08:35:31,132 INFO: -----------------SAVE:16epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:16/loss:2.9658/f1:0.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:24<00:00,  5.36s/it]\n",
      "2022-06-04 09:23:55,517 INFO: Epoch:[017/050]\n",
      "2022-06-04 09:23:55,518 INFO: Train Loss:3.198 | Acc:0.2001 | F1:0.1952\n",
      "2022-06-04 09:29:58,783 INFO: val Loss:2.802 | Acc:0.2430 | F1:0.2453\n",
      "2022-06-04 09:29:59,939 INFO: -----------------SAVE:17epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:17/loss:2.8023/f1:0.2453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:52<00:00,  5.41s/it]\n",
      "2022-06-04 10:18:52,782 INFO: Epoch:[018/050]\n",
      "2022-06-04 10:18:52,783 INFO: Train Loss:3.082 | Acc:0.2153 | F1:0.2096\n",
      "2022-06-04 10:24:58,515 INFO: val Loss:2.592 | Acc:0.2904 | F1:0.2773\n",
      "2022-06-04 10:24:59,909 INFO: -----------------SAVE:18epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:18/loss:2.5919/f1:0.2773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:40<00:00,  5.39s/it]\n",
      "2022-06-04 11:13:40,212 INFO: Epoch:[019/050]\n",
      "2022-06-04 11:13:40,213 INFO: Train Loss:3.043 | Acc:0.2306 | F1:0.2252\n",
      "2022-06-04 11:19:44,656 INFO: val Loss:2.575 | Acc:0.3018 | F1:0.2855\n",
      "2022-06-04 11:19:46,225 INFO: -----------------SAVE:19epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:19/loss:2.5753/f1:0.2855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:46<00:00,  5.40s/it]\n",
      "2022-06-04 12:08:32,836 INFO: Epoch:[020/050]\n",
      "2022-06-04 12:08:32,837 INFO: Train Loss:3.003 | Acc:0.2342 | F1:0.2287\n",
      "2022-06-04 12:14:37,035 INFO: val Loss:2.446 | Acc:0.3298 | F1:0.3128\n",
      "2022-06-04 12:14:38,471 INFO: -----------------SAVE:20epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:20/loss:2.4463/f1:0.3128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:43<00:00,  5.39s/it]\n",
      "2022-06-04 13:03:22,084 INFO: Epoch:[021/050]\n",
      "2022-06-04 13:03:22,085 INFO: Train Loss:2.946 | Acc:0.2460 | F1:0.2403\n",
      "2022-06-04 13:09:27,698 INFO: val Loss:2.377 | Acc:0.3316 | F1:0.3153\n",
      "2022-06-04 13:09:29,130 INFO: -----------------SAVE:21epoch----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:21/loss:2.3765/f1:0.3153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 542/542 [48:35<00:00,  5.38s/it]\n",
      "2022-06-04 13:58:04,698 INFO: Epoch:[022/050]\n",
      "2022-06-04 13:58:04,699 INFO: Train Loss:2.867 | Acc:0.2614 | F1:0.2551\n",
      "2022-06-04 14:04:11,274 INFO: val Loss:2.665 | Acc:0.2965 | F1:0.3007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best epoch:21/loss:2.3765/f1:0.3153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|██████████████████▍                      | 244/542 [21:54<26:29,  5.34s/it]"
     ]
    }
   ],
   "source": [
    "# fold 없이\n",
    "models_path = []\n",
    "args.fold = 0\n",
    "args.exp_num = str(0)\n",
    "save_path = main(args)\n",
    "models_path.append(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a866b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold X\n",
    "result = result(models_path, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34beddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용한 모델의 batch_size, epoch, img_size, patience\n",
    "print('batch_size =', args.batch_size)\n",
    "print('epochs =', args.epochs)\n",
    "print('img_size =', args.img_size)\n",
    "print('patience =', args.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model =', args.encoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ee487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f94a302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31aecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import logging  # 로그 출력\n",
    "import easydict  # 속성으로 dict 값에 access할 수 있음\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # process bar\n",
    "from os.path import join as opj\n",
    "from ptflops import get_model_complexity_info\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, grad_scaler\n",
    "from torchvision import transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Config\n",
    "\n",
    "Hyper-parameter 정의\n",
    "\n",
    "args = easydict.EasyDict(\n",
    "    {'exp_num':'0',\n",
    "     \n",
    "     # Path settings\n",
    "     # /home/lab18/Data/product_image/Training/image/10268_아넬라사과디저트_2입/\n",
    "     'data_path':'/home/lab16/jupyter_home/Data/product_image/',\n",
    "     'Kfold':1,\n",
    "     'model_path':'results/',\n",
    "\n",
    "     # Model parameter settings \n",
    "     'encoder_name':'regnety_064',\n",
    "     'drop_path_rate':0.2,\n",
    "     \n",
    "     # Training parameter settings\n",
    "     ## Base Parameter\n",
    "     'img_size':224,\n",
    "     'batch_size':16,\n",
    "     'epochs':50,\n",
    "     'optimizer':'Lamb',\n",
    "#      'optimizer':'Adadelta',\n",
    "     'initial_lr':5e-6,\n",
    "     'weight_decay':1e-3,\n",
    "\n",
    "     ## Augmentation\n",
    "     'aug_ver':2,\n",
    "\n",
    "     ## Scheduler (OnecycleLR)\n",
    "     'scheduler':'cycle',\n",
    "     'warm_epoch':5,\n",
    "     'max_lr':1e-3,\n",
    "\n",
    "     ### Cosine Annealing\n",
    "     'min_lr':5e-6,\n",
    "     'tmax':145,\n",
    "\n",
    "     ## etc.\n",
    "     'patience':50,\n",
    "     'clipping':None,\n",
    "\n",
    "     # Hardware settings\n",
    "     'amp':True,\n",
    "     'multi_gpu':False,\n",
    "     'logging':False,\n",
    "     'num_workers':0,\n",
    "     'seed':42\n",
    "    })\n",
    "\n",
    "# Utils for training and Logging\n",
    "\n",
    "# Warmup Learning rate scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimizer(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        \n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]\n",
    "\n",
    "# Logging\n",
    "def get_root_logger(logger_name='basicsr',\n",
    "                    log_level=logging.INFO,\n",
    "                    log_file=None):\n",
    "\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    # if the logger has been initialized, just return it\n",
    "    if logger.hasHandlers():\n",
    "        return logger\n",
    "\n",
    "    format_str = '%(asctime)s %(levelname)s: %(message)s'\n",
    "    logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "    if log_file is not None:\n",
    "        file_handler = logging.FileHandler(log_file, 'w')\n",
    "        file_handler.setFormatter(logging.Formatter(format_str))\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AvgMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)\n",
    "\n",
    "\n",
    "\n",
    "# Dataset & Loader\n",
    "\n",
    "# from torchvision.datasets import ImageFolder\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "\n",
    "import random\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 레이블을 one-hot-vector로 변환\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# targets = le.fit_transform(y)\n",
    "# targets = torch.as_tensor(targets)\n",
    "# one_hot_y = F.one_hot(targets)\n",
    "\n",
    "# validation용 추가\n",
    "class Valid_Dataset(Dataset):\n",
    "    def __init__(self, transform=None): \n",
    "        total_images_path = glob('/home/lab16/jupyter_home/Data/product_image/Validation/image/**/*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.file_name = file_names\n",
    "        \n",
    "        each_label = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            each_label.append(os.path.basename(total_images_path[i])[:5])\n",
    "            \n",
    "        # 레이블을 one-hot-vector로 변환\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        targets = le.fit_transform(each_label)\n",
    "        targets = torch.as_tensor(targets)\n",
    "#         one_hot_y = F.one_hot(targets)\n",
    "            \n",
    "        self.target = np.array(targets) # 목표는 label\n",
    "#         self.target = one_hot_y\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Validation Dataset size:{len(self.file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx):  # train 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/train_256_new/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Validation/total_image/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        target = self.target[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "        # HWC => CHW-layout 변환\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "class Train_Dataset(Dataset):\n",
    "#     def __init__(self, df, transform=None):\n",
    "    def __init__(self, transform=None):\n",
    "#         self.file_name = df['file_name'].values      \n",
    "        \n",
    "        total_images_path = glob('/home/lab16/jupyter_home/Data/product_image/Training/image/**/*.jpg')\n",
    "        file_names = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            file_names.append(os.path.basename(total_images_path[i]))\n",
    "        file_names = np.array(file_names)\n",
    "\n",
    "        self.file_name = file_names\n",
    "                \n",
    "        each_label = []\n",
    "        for i in range(len(total_images_path)):\n",
    "            each_label.append(os.path.basename(total_images_path[i])[:5])\n",
    "\n",
    "        # 레이블을 one-hot-vector로 변환\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        targets = le.fit_transform(each_label)\n",
    "        targets = torch.as_tensor(targets)\n",
    "#         one_hot_y = F.one_hot(targets)\n",
    "            \n",
    "        self.target = np.array(targets) # 목표는 label\n",
    "#         self.target = one_hot_y\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Training Dataset size:{len(self.file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx):  # train 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/train_256_new/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Training/total_image/', self.file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        target = self.target[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "        # HWC => CHW-layout 변환\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_name)\n",
    "\n",
    "class Test_dataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.test_file_name = df['file_name'].values\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f'Test Dataset size:{len(self.test_file_name)}')\n",
    "\n",
    "    def __getitem__(self, idx): # test 경로에 있는 png 이미지 읽어서 float32로 변환\n",
    "#         image = cv2.imread(opj('./data/test_256_new/', self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.imread(opj('/home/lab16/jupyter_home/Data/product_image/Test/total_image/', self.test_file_name[idx])).astype(np.float32)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # BGR=>RGB 변환\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(torch.from_numpy(image.transpose(2,0,1)))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_file_name)\n",
    "\n",
    "# def get_loader(df, phase: str, batch_size, shuffle, num_workers, transform):\n",
    "def get_loader(phase: str, batch_size, shuffle, num_workers, transform):\n",
    "    if phase == 'test':\n",
    "#         dataset = Test_dataset(df, transform)  \n",
    "        dataset = Test_dataset(transform) \n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "    # 임시로 추가\n",
    "    elif phase == 'validation':\n",
    "#         dataset = Train_Dataset(df, transform)\n",
    "        dataset = Valid_Dataset(transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "    else:\n",
    "#         dataset = Train_Dataset(df, transform)\n",
    "        path = ''\n",
    "        dataset = Train_Dataset(transform)\n",
    "        data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True,\n",
    "                                 drop_last=False)\n",
    "        \n",
    "    return data_loader\n",
    "\n",
    "def get_train_augmentation(img_size, ver):\n",
    "    if ver == 1: # for validset\n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "#                 transforms.ToTensor()\n",
    "                ])\n",
    "\n",
    "    if ver == 2:\n",
    "        transform = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomPerspective(),\n",
    "                transforms.RandomAffine((20)),  # x, y축으로 이미지 늘림\n",
    "                transforms.RandomRotation(90),\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "#                 transforms.ToTensor()\n",
    "            ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "\n",
    "\n",
    "# Network\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        # 사전 학습된 모델 사용하기\n",
    "        self.encoder = timm.create_model(args.encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=args.drop_path_rate,\n",
    "                                    )\n",
    "        \n",
    "        if 'regnet' in args.encoder_name:        \n",
    "            num_head = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Linear(num_head, 76)\n",
    "        \n",
    "        elif 'efficient' in args.encoder_name:\n",
    "            num_head = self.encoder.classifier.in_features\n",
    "            self.encoder.classifier = nn.Linear(num_head, 76)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "class Network_test(nn.Module):\n",
    "    def __init__(self, encoder_name):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model(encoder_name, pretrained=True,\n",
    "                                    drop_path_rate=0,\n",
    "                                    )\n",
    "        \n",
    "        if 'regnet' in encoder_name:        \n",
    "            num_head = self.encoder.head.fc.in_features\n",
    "            self.encoder.head.fc = nn.Linear(num_head, 76)\n",
    "        \n",
    "        elif 'efficient' in encoder_name:\n",
    "            num_head = self.encoder.classifier.in_features\n",
    "            self.encoder.classifier = nn.Linear(num_head, 76)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Trainer for Training & Validation\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, args, save_path):\n",
    "        '''\n",
    "        args: arguments\n",
    "        save_path: Model 가중치 저장 경로\n",
    "        '''\n",
    "        super(Trainer, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f'device:{self.device}')\n",
    "\n",
    "        # Logging\n",
    "        log_file = os.path.join(save_path, 'log_0603.log')\n",
    "        self.logger = get_root_logger(logger_name='IR', log_level=logging.INFO, log_file=log_file)\n",
    "        self.logger.info(args)\n",
    "        # self.logger.info(args.tag)\n",
    "\n",
    "        # Train, Valid Set load\n",
    "        ############################################################################\n",
    "#         df_train = pd.read_csv(opj(args.data_path, 'train_df.csv'))\n",
    "#         print('Read train_df.csv')\n",
    "\n",
    "#         kf = StratifiedKFold(n_splits=args.Kfold, shuffle=True, random_state=args.seed)\n",
    "#         for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(df_train)), y=df_train['label'])):\n",
    "#             df_train.loc[val_idx, 'fold'] = fold\n",
    "#         val_idx = list(df_train[df_train['fold'] == int(args.fold)].index)\n",
    "\n",
    "#         df_val = df_train[df_train['fold'] == args.fold].reset_index(drop=True)\n",
    "#         df_train = df_train[df_train['fold'] != args.fold].reset_index(drop=True)\n",
    "\n",
    "        # Augmentation\n",
    "        self.train_transform = get_train_augmentation(img_size=args.img_size, ver=args.aug_ver)\n",
    "        self.test_transform = get_train_augmentation(img_size=args.img_size, ver=1)\n",
    "        \n",
    "        # 수정 - TrainLoader\n",
    "        self.train_loader = get_loader(phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        self.val_loader = get_loader(phase='validation', batch_size=args.batch_size, shuffle=False,\n",
    "                                       num_workers=args.num_workers, transform=self.train_transform)\n",
    "        \n",
    "        # TrainLoader\n",
    "#         self.train_loader = get_loader(df_train, phase='train', batch_size=args.batch_size, shuffle=True,\n",
    "#                                        num_workers=args.num_workers, transform=self.train_transform)\n",
    "#         self.val_loader = get_loader(df_val, phase='train', batch_size=args.batch_size, shuffle=False,\n",
    "#                                        num_workers=args.num_workers, transform=self.test_transform)\n",
    "\n",
    "        # Network\n",
    "        self.model = Network(args).to(self.device)\n",
    "        macs, params = get_model_complexity_info(self.model, (3, args.img_size, args.img_size), as_strings=True,\n",
    "                                                 print_per_layer_stat=False, verbose=False)\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        self.logger.info('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "        # Loss\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer & Scheduler\n",
    "        self.optimizer = optim.Lamb(self.model.parameters(), lr=args.initial_lr, weight_decay=args.weight_decay)\n",
    "#         self.optimizer = optim.Adadelta(self.model.parameters())\n",
    "        \n",
    "        iter_per_epoch = len(self.train_loader)\n",
    "        self.warmup_scheduler = WarmUpLR(self.optimizer, iter_per_epoch * args.warm_epoch)\n",
    "\n",
    "        if args.scheduler == 'step':\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=args.milestone, gamma=args.lr_factor, verbose=True)\n",
    "        elif args.scheduler == 'cos':\n",
    "            tmax = args.tmax # half-cycle \n",
    "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max = tmax, eta_min=args.min_lr, verbose=True)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "        if args.multi_gpu:\n",
    "            self.model = nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "        # Train / Validate\n",
    "        best_loss = np.inf\n",
    "        best_acc = 0\n",
    "        best_epoch = 0\n",
    "        early_stopping = 0\n",
    "        start = time.time()\n",
    "        for epoch in range(1, args.epochs+1):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            if args.scheduler == 'cos':\n",
    "                if epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc, train_f1 = self.training(args)\n",
    "\n",
    "            # Model weight in Multi_GPU or Single GPU\n",
    "            state_dict= self.model.module.state_dict() if args.multi_gpu else self.model.state_dict()\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc, val_f1 = self.validate(args, phase='val')\n",
    "\n",
    "            # Save models\n",
    "            if val_loss < best_loss:\n",
    "                early_stopping = 0\n",
    "                best_epoch = epoch\n",
    "                best_loss = val_loss\n",
    "                best_acc = val_acc\n",
    "                best_f1 = val_f1\n",
    "\n",
    "                torch.save({'epoch':epoch,\n",
    "                            'state_dict':state_dict,\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'scheduler': self.scheduler.state_dict(),\n",
    "                    }, os.path.join(save_path, 'best_model.pth'))\n",
    "                self.logger.info(f'-----------------SAVE:{best_epoch}epoch----------------')\n",
    "            else:\n",
    "                early_stopping += 1\n",
    "\n",
    "            # Early Stopping\n",
    "            if early_stopping == args.patience:\n",
    "                break\n",
    "                \n",
    "            print(f'\\nbest epoch:{best_epoch}/loss:{best_loss:.4f}/f1:{best_f1:.4f}')\n",
    "\n",
    "        self.logger.info(f'\\nBest Val Epoch:{best_epoch} | Val Loss:{best_loss:.4f} | Val Acc:{best_acc:.4f} | Val F1:{best_f1:.4f}')\n",
    "        end = time.time()\n",
    "        self.logger.info(f'Total Process time:{(end - start) / 60:.3f}Minute')\n",
    "\n",
    "    # Training\n",
    "    def training(self, args):\n",
    "        self.model.train()\n",
    "        train_loss = AvgMeter()\n",
    "        train_acc = 0\n",
    "        preds_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        scaler = grad_scaler.GradScaler()\n",
    "        for i, (images, targets) in enumerate(tqdm(self.train_loader)):\n",
    "            images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "            targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "            \n",
    "            if self.epoch <= args.warm_epoch:\n",
    "                self.warmup_scheduler.step()\n",
    "\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    preds = self.model(images)\n",
    "                    loss = self.criterion(preds, targets)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Gradient Clipping\n",
    "                if args.clipping is not None:\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            else:\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), args.clipping)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if args.scheduler == 'cycle':\n",
    "                if self.epoch > args.warm_epoch:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "            # Metric\n",
    "            train_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "            preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "            targets_list.extend(targets.cpu().detach().numpy())\n",
    "            # log\n",
    "            train_loss.update(loss.item(), n=images.size(0))\n",
    "\n",
    "        train_acc /= len(self.train_loader.dataset)\n",
    "        train_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "        self.logger.info(f'Epoch:[{self.epoch:03d}/{args.epochs:03d}]')\n",
    "        self.logger.info(f'Train Loss:{train_loss.avg:.3f} | Acc:{train_acc:.4f} | F1:{train_f1:.4f}')\n",
    "        return train_loss.avg, train_acc, train_f1\n",
    "            \n",
    "    # Validation or Dev\n",
    "    def validate(self, args, phase='val'):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = AvgMeter()\n",
    "            val_acc = 0\n",
    "            preds_list = []\n",
    "            targets_list = []\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.val_loader):\n",
    "                images = torch.tensor(images, device=self.device, dtype=torch.float32)\n",
    "                targets = torch.tensor(targets, device=self.device, dtype=torch.long)\n",
    "\n",
    "                preds = self.model(images)\n",
    "                loss = self.criterion(preds, targets)\n",
    "\n",
    "                # Metric\n",
    "                val_acc += (preds.argmax(dim=1) == targets).sum().item()\n",
    "                preds_list.extend(preds.argmax(dim=1).cpu().detach().numpy())\n",
    "                targets_list.extend(targets.cpu().detach().numpy())\n",
    "\n",
    "                # log\n",
    "                val_loss.update(loss.item(), n=images.size(0))\n",
    "            val_acc /= len(self.val_loader.dataset)\n",
    "            val_f1 = f1_score(np.array(targets_list), np.array(preds_list), average='macro')\n",
    "\n",
    "            self.logger.info(f'{phase} Loss:{val_loss.avg:.3f} | Acc:{val_acc:.4f} | F1:{val_f1:.4f}')\n",
    "        return val_loss.avg, val_acc, val_f1\n",
    "\n",
    "# Main\n",
    "\n",
    "def main(args):\n",
    "    print('<---- Training Params ---->')\n",
    "    \n",
    "    # Random Seed\n",
    "    seed = args.seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    save_path = os.path.join(args.model_path, (args.exp_num).zfill(3))\n",
    "    \n",
    "    # Create model directory\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    Trainer(args, save_path)\n",
    "\n",
    "    return save_path\n",
    "\n",
    "# Inference\n",
    "\n",
    "def predict(encoder_name, test_loader, device, model_path):\n",
    "    model = Network_test(encoder_name).to(device)\n",
    "    model.load_state_dict(torch.load(opj(model_path, 'best_model.pth'))['state_dict'])\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            images = torch.as_tensor(images, device=device, dtype=torch.float32)\n",
    "            preds = model(images)\n",
    "            preds = torch.softmax(preds, dim=1)\n",
    "            preds_list.extend(preds.cpu().tolist())\n",
    "\n",
    "    return np.array(preds_list)\n",
    "\n",
    "def ensemble_5fold(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_064', \n",
    "                             test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = (predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "# fold X\n",
    "def result(model_path_list, test_loader, device):\n",
    "    predict_list = []\n",
    "    for model_path in model_path_list:\n",
    "        prediction = predict(encoder_name= 'regnety_064', \n",
    "                             test_loader = test_loader, device = device, model_path = model_path)\n",
    "        predict_list.append(prediction)\n",
    "    ensemble = predict_list[0]\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "# Train & Inference\n",
    "\n",
    "img_size = 224\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# sub = pd.read_csv('./data/sample_submission.csv')\n",
    "# df_train = pd.read_csv('./data/train_df.csv')\n",
    "# df_test = pd.read_csv('./data/test_df.csv')\n",
    "\n",
    "# test_transform = get_train_augmentation(img_size=img_size, ver=1)\n",
    "# test_dataset = Test_dataset(df_test, test_transform)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fold 없이\n",
    "models_path = []\n",
    "args.fold = 0\n",
    "args.exp_num = str(0)\n",
    "save_path = main(args)\n",
    "models_path.append(save_path)\n",
    "\n",
    "models_path\n",
    "\n",
    "# fold X\n",
    "result = result(models_path, test_loader, device)\n",
    "\n",
    "# 학습에 사용한 모델의 batch_size, epoch, img_size, patience\n",
    "print('batch_size =', args.batch_size)\n",
    "print('epochs =', args.epochs)\n",
    "print('img_size =', args.img_size)\n",
    "print('patience =', args.patience)\n",
    "\n",
    "print('model =', args.encoder_name)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_16] *",
   "language": "python",
   "name": "conda-env-pytorch_16-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
