{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c3de41",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d30ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "from itertools import product\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c81cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e643018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':256,\n",
    "    'EPOCHS':50,\n",
    "    'PATIENCE':10,\n",
    "    'class':14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb70e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_path = '/home/lab17/jupyter_home/Data/product_image/Training/'\n",
    "Valid_path = '/home/lab17/jupyter_home/Data/product_image/Validation/'\n",
    "Test_path = '/home/lab17/jupyter_home/git/test_img/'\n",
    "save_graph_path = './result/'\n",
    "save_model_path = '/home/lab17/jupyter_home/saved_models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c340a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca14fa",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caabde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(data_dir):\n",
    "#     img_path_list = []\n",
    "    label_list = []\n",
    "    label_name_list = []\n",
    "    \n",
    "    image_path = os.path.join(data_dir, 'dessert')\n",
    "    \n",
    "    for product_name in os.listdir(image_path):\n",
    "        product_path = os.path.join(image_path, product_name)\n",
    "        if os.path.isdir(product_path):\n",
    "#             get image path\n",
    "#             img_path_list.extend(glob(os.path.join(product_path, '*.jpg')))\n",
    "#             img_path_list.extend(glob(os.path.join(product_path, '*.png')))\n",
    "            label = list(product_name[:5])\n",
    "            name = product_name[6:]\n",
    "            \n",
    "            # get label\n",
    "            label_list.append(''.join(label))\n",
    "            label_name_list.append(name)\n",
    "                \n",
    "#     return img_path_list, label_list\n",
    "    return label_list, label_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b750e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list, label_name_list = get_train_data(Train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fc35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--targets\n",
      " [12 10  2  8  0  3 11  9  4  7 13  5  6  1]\n",
      "{'55701': 12, '45661': 10, '35211': 2, '45659': 8, '25222': 0, '35584': 3, '55034': 11, '45660': 9, '35585': 4, '45658': 7, '55702': 13, '45030': 5, '45657': 6, '25228': 1}\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(label_list)\n",
    "print('--targets\\n' , targets)\n",
    "\n",
    "label_encoder = {key:val for key, val in zip(label_list, targets)}\n",
    "print(label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bdf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "label_decoder = {v: k for k, v in label_encoder.items()}\n",
    "label_name_decoder = {key : value for key, value in zip(label_list, label_name_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f7ecf",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0584b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_data(data_dir):\n",
    "    img_valid_list = []\n",
    "    label_valid_list = []\n",
    "    \n",
    "    image_path = os.path.join(data_dir, 'dessert')\n",
    "    \n",
    "    for product_name in os.listdir(image_path):\n",
    "        product_path = os.path.join(image_path, product_name)\n",
    "        if os.path.isdir(product_path):\n",
    "            # get image path\n",
    "            img_valid_list.extend(glob(os.path.join(product_path, '*.jpg')))\n",
    "            img_valid_list.extend(glob(os.path.join(product_path, '*.png')))\n",
    "            label = list(product_name[:5])\n",
    "            \n",
    "            # get label\n",
    "            label_valid_list.append(''.join(label))\n",
    "                \n",
    "    return img_valid_list, label_valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c37a8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_data_blanced(img, label):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(CFG['class']):\n",
    "        _img = img[(i * 15): ((i + 1) * 15)]\n",
    "        _label = label[i]\n",
    "        \n",
    "        for img_product in _img:\n",
    "            x.append(img_product)\n",
    "            y.append(_label)\n",
    "            \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72826760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_valid_list, label_valid_list = get_valid_data(Valid_path)\n",
    "x_valid, y_valid = valid_data_blanced(img_valid_list, label_valid_list)\n",
    "len(label_valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4795154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([210, 14])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le2 = preprocessing.LabelEncoder()\n",
    "targets_y = le2.fit_transform(y_valid)\n",
    "targets_y_t = torch.as_tensor(targets_y)\n",
    "one_hot_valid_y = F.one_hot(targets_y_t)\n",
    "one_hot_valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ad3ff",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f87f431a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(data_dir):\n",
    "    img_path_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    image_path = data_dir\n",
    "    \n",
    "#     for product in os.listdir(image_path):\n",
    "\n",
    "    # get image path\n",
    "    img_path_list.extend(glob(os.path.join(image_path, '*.jpg')))\n",
    "    img_path_list.extend(glob(os.path.join(image_path, '*.png')))\n",
    "    label_list = [ip[len('/home/lab17/jupyter_home/git/test_img/'):-6] for ip in img_path_list]\n",
    "\n",
    "    # get label\n",
    "#     label_list.append(''.join(label))\n",
    "                \n",
    "    return img_path_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15f3cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path, test_label_list = get_test_data(Test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6076d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_x = test_img_path\n",
    "# 레이블을 one-hot-vector로 변환\n",
    "test_y = [label_encoder[key] for key in test_label_list]\n",
    "test_targets = torch.as_tensor(test_y)\n",
    "one_hot_test_y = F.one_hot(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9280e427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['55034, 11', '45661, 10', '25228, 1', '25222, 0', '45659, 8', '55701, 12', '45030, 5', '35211, 2', '45660, 9', '45659, 8', '35585, 4', '55702, 13', '25222, 0', '55702, 13', '35211, 2', '45030, 5', '45661, 10', '35584, 3', '25222, 0', '55701, 12', '35211, 2', '35211, 2', '55701, 12', '35584, 3', '45660, 9', '55702, 13', '25222, 0', '45657, 6', '45657, 6', '55701, 12', '45657, 6']\n"
     ]
    }
   ],
   "source": [
    "print([f'{i}, {y}' for i, y in zip(test_label_list, test_y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fd5d0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b00271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsCustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True, transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.train_mode = train_mode\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        # Get image data\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        # By default OpenCV uses BGR color space for color images,\n",
    "        # so we need to convert the image to RGB color space.\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.train_mode:\n",
    "#             image = image.astype(np.int16)\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            augmented = self.transforms(image=image)\n",
    "            image = augmented['image']\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "A_test_transform = albumentations.Compose([\n",
    "                                    A.Resize(256, 256),\n",
    "                                    A.Normalize(mean=(0.744859, 0.735139, 0.711357), std=(0.100712, 0.120692, 0.167998)),  \n",
    "#                                     A.pytorch.transforms.ToTensor(),\n",
    "                                    A.pytorch.transforms.ToTensorV2(transpose_mask=True),\n",
    "                                ])\n",
    "\n",
    "A_vali_dataset = AlbumentationsCustomDataset(x_valid, one_hot_valid_y, train_mode=True, transforms=A_test_transform)\n",
    "A_vali_loader = DataLoader(A_vali_dataset, batch_size = 5, shuffle=False, num_workers=0, collate_fn=None)\n",
    "\n",
    "A_test_dataset = AlbumentationsCustomDataset(test_x, one_hot_test_y, train_mode=False, transforms=A_test_transform)\n",
    "A_test_loader = DataLoader(A_test_dataset, batch_size = 4, shuffle=False, num_workers=0, collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dddfe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(2048, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000,CFG['class'])\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # x = x.mean(dim=(-2, -1))\n",
    "        # (batch, 2048, 4, 4)\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "#         out = self.softmax(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75c0cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetb4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetb4, self).__init__()\n",
    "        model = models.efficientnet_b4(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(1792, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, CFG['class'])\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # (batch, 1792, 1, 1)\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "#         out = self.softmax(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71e4f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegNet, self).__init__()\n",
    "        model = models.regnet_y_16gf(pretrained=True)\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.feature_extract = nn.Sequential(*modules)\n",
    "        self.fc1 = nn.Linear(3024, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000, CFG['class'])\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # (batch, 3024, 1, 1)\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "#         out = self.softmax(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf06d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(real, pred):\n",
    "    score = accuracy_score(real, pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5009cd",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "300f3c7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8023d787c4446fb2dfec89e02edf98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 1.6851365715265274\n",
      "test acc : 0.41935483870967744\n"
     ]
    }
   ],
   "source": [
    "pred_ensemble = []\n",
    "batch_size = 34\n",
    "\n",
    "#---------\n",
    "model_name = 'ResNet50'\n",
    "model_lr = '0.0001'\n",
    "model_optim = 'adam'\n",
    "model_sch = 'CosineAnnealing' \n",
    "#---------\n",
    "\n",
    "model_test = ResNet50().to(device)\n",
    "model_test.load_state_dict(torch.load('/home/lab17/jupyter_home/saved_models/{}_{}_{}_{}_example.pth'.format(model_name, model_lr, model_optim, model_sch)))\n",
    "model_test.eval()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# test_loss = []\n",
    "# f_pred = []\n",
    "\n",
    "# for img, label in tqdm(iter(A_vali_loader)):\n",
    "#     img, label = img.float().to(device), label.float().to(device)\n",
    "    \n",
    "#     # Data -> Model -> Output\n",
    "#     logit = model_test(img)\n",
    "#     logit = torch.squeeze(logit)\n",
    "    \n",
    "#     # Calc loss\n",
    "#     loss = criterion(logit, label)\n",
    "\n",
    "#     test_loss.append(loss.item())\n",
    "#     f_pred.extend(logit.argmax(1).detach().cpu().numpy().tolist())\n",
    "    \n",
    "# print('val loss :' ,np.mean(test_loss))\n",
    "# print('val acc :', score_function(targets_y, f_pred))\n",
    "\n",
    "test_loss = []\n",
    "f_pred = []\n",
    "\n",
    "for img, label in tqdm(iter(A_test_loader)):\n",
    "    img, label = img.float().to(device), label.float().to(device)\n",
    "    \n",
    "    # Data -> Model -> Output\n",
    "    logit = model_test(img)\n",
    "    logit = torch.squeeze(logit)\n",
    "    \n",
    "    # Calc loss\n",
    "    loss = criterion(logit, label)\n",
    "\n",
    "    test_loss.append(loss.item())\n",
    "    f_pred.extend(logit.argmax(1).detach().cpu().numpy().tolist())\n",
    "    \n",
    "print('test loss :' ,np.mean(test_loss))\n",
    "print('test acc :', score_function(test_y, f_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b0a83",
   "metadata": {},
   "source": [
    "- patience 너무커서 과적합,?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3e81827",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_result = [label_name_decoder[label_decoder[result]] for result in f_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6861970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test이미지 번호 | 정답 | 예측\n",
      "x 55034_1, 돌트로피칼666G, 돌황도666G\n",
      "o 45661_1, 씨제이)쁘티첼(요거젤리블루베리), 씨제이)쁘티첼(요거젤리블루베리)\n",
      "x 25228_1, 대만)파인애플케익184G, 대만)망고케익184g\n",
      "x 25222_3, 대만)망고케익184g, 돌황도666G\n",
      "x 45659_2, 씨제이)쁘티첼(요거젤리딸기), 돌황도666G\n",
      "o 55701_3, 쁘띠첼요거젤리밀감, 쁘띠첼요거젤리밀감\n",
      "o 45030_2, 돌황도666G, 돌황도666G\n",
      "x 35211_2, 매일유업)데르뜨130G, 돌황도666G\n",
      "x 45660_2, 씨제이)쁘티첼(요거젤리화이트코코), 씨제이)쁘티첼(요거젤리밀감)\n",
      "x 45659_1, 씨제이)쁘티첼(요거젤리딸기), 대만)망고케익184g\n",
      "o 35585_1, 매일데르뜨감귤90G, 매일데르뜨감귤90G\n",
      "x 55702_2, 쁘띠첼요거젤리복숭아, 씨제이)쁘티첼(요거젤리복숭아)\n",
      "x 25222_1, 대만)망고케익184g, 돌황도666G\n",
      "x 55702_3, 쁘띠첼요거젤리복숭아, 씨제이)쁘티첼(요거젤리복숭아)\n",
      "x 35211_1, 매일유업)데르뜨130G, 돌황도666G\n",
      "o 45030_1, 돌황도666G, 돌황도666G\n",
      "x 45661_2, 씨제이)쁘티첼(요거젤리블루베리), 돌황도666G\n",
      "x 35584_2, 매일데르뜨파인애플90G, 돌황도666G\n",
      "x 25222_4, 대만)망고케익184g, 돌황도666G\n",
      "x 55701_2, 쁘띠첼요거젤리밀감, 씨제이)쁘티첼(요거젤리밀감)\n",
      "x 35211_4, 매일유업)데르뜨130G, 씨제이)쁘티첼(요거젤리딸기)\n",
      "o 35211_3, 매일유업)데르뜨130G, 매일유업)데르뜨130G\n",
      "o 55701_1, 쁘띠첼요거젤리밀감, 쁘띠첼요거젤리밀감\n",
      "o 35584_1, 매일데르뜨파인애플90G, 매일데르뜨파인애플90G\n",
      "o 45660_1, 씨제이)쁘티첼(요거젤리화이트코코), 씨제이)쁘티첼(요거젤리화이트코코)\n",
      "x 55702_1, 쁘띠첼요거젤리복숭아, 씨제이)쁘티첼(요거젤리복숭아)\n",
      "o 25222_2, 대만)망고케익184g, 대만)망고케익184g\n",
      "o 45657_1, 씨제이)쁘티첼(요거젤리복숭아), 씨제이)쁘티첼(요거젤리복숭아)\n",
      "x 45657_3, 씨제이)쁘티첼(요거젤리복숭아), 돌황도666G\n",
      "o 55701_4, 쁘띠첼요거젤리밀감, 쁘띠첼요거젤리밀감\n",
      "o 45657_2, 씨제이)쁘티첼(요거젤리복숭아), 씨제이)쁘티첼(요거젤리복숭아)\n"
     ]
    }
   ],
   "source": [
    "print('test이미지 번호 | 정답 | 예측')\n",
    "for img, res in zip(test_img_path, f_result):\n",
    "    if label_name_decoder[img[-11:-6]]==res:\n",
    "        print(f'o {img[-11:-4]}, {label_name_decoder[img[-11:-6]]}, {res}')\n",
    "    else:\n",
    "        print(f'x {img[-11:-4]}, {label_name_decoder[img[-11:-6]]}, {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aea86d",
   "metadata": {},
   "source": [
    "# Inference2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "387f3ac9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/lab17/jupyter_home/saved_models/EfficientNetb4_0.0001_Lamb_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/EfficientNetb4_0.0001_adam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/EfficientNetb4_0.0001_nadam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/EfficientNetb4_0.0001_rmsprop_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/EfficientNetb4_0.001_Lamb_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/EfficientNetb4_0.001_adam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/EfficientNetb4_0.001_nadam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/EfficientNetb4_0.001_rmsprop_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/EfficientNetb4_1e-05_rmsprop_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_0.0001_Lamb_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_0.0001_adam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_0.0001_nadam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_0.0001_rmsprop_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_0.001_Lamb_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_0.001_adam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_0.001_nadam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_0.001_rmsprop_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_1e-05_Lamb_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_1e-05_adam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_1e-05_nadam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/RegNet_1e-05_rmsprop_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_0.0001_Lamb_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_0.0001_adam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_0.0001_nadam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_0.0001_rmsprop_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_0.001_Lamb_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_0.001_adam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_0.001_nadam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_0.001_rmsprop_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_1e-05_Lamb_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_1e-05_adam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_1e-05_nadam_CosineAnnealing_example.pth',\n",
       " '/home/lab17/jupyter_home/saved_models/ResNet50_1e-05_rmsprop_CosineAnnealing_example.pth']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(os.listdir('/home/lab17/jupyter_home/saved_models/'))\n",
    "file_list = sorted(glob('/home/lab17/jupyter_home/saved_models/*.pth'))\n",
    "print(len(file_list))\n",
    "sorted(glob('/home/lab17/jupyter_home/saved_models/*.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e92b95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=['/home/lab17/jupyter_home/saved_models/RegNet_0.001_rmsprop_CosineAnnealing_example.pth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d024171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a671284e4454958885e1bc654441436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegNet_0.001_rmsprop_CosineAnnealing_example.pth\n",
      "test loss : 4.247981650754809\n",
      "test acc : 0.3870967741935484\n"
     ]
    }
   ],
   "source": [
    "pred_ensemble = []\n",
    "batch_size = 34\n",
    "\n",
    "for m_path in file_list:\n",
    "    if 'ResNet50' in m_path:\n",
    "        model_test = ResNet50().to(device)\n",
    "    elif 'EfficientNetb4' in m_path:\n",
    "        model_test = EfficientNetb4().to(device)\n",
    "    elif 'RegNet' in m_path:\n",
    "        model_test = RegNet().to(device)    \n",
    "\n",
    "    model_test.load_state_dict(torch.load(m_path))\n",
    "    model_test.eval()\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    test_loss = []\n",
    "    f_pred = []\n",
    "\n",
    "#     for img, label in tqdm(iter(A_vali_loader)):\n",
    "#         img, label = img.float().to(device), label.float().to(device)\n",
    "\n",
    "#         # Data -> Model -> Output\n",
    "#         logit = model_test(img)\n",
    "#         logit = torch.squeeze(logit)\n",
    "\n",
    "#         # Calc loss\n",
    "#         loss = criterion(logit, label)\n",
    "\n",
    "#         test_loss.append(loss.item())\n",
    "#         f_pred.extend(logit.argmax(1).detach().cpu().numpy().tolist())\n",
    "\n",
    "#     print('val loss :' ,np.mean(test_loss))\n",
    "#     print('val acc :', score_function(targets_y, f_pred))\n",
    "\n",
    "#     test_loss = []\n",
    "#     f_pred = []\n",
    "\n",
    "    for img, label in tqdm(iter(A_test_loader)):\n",
    "        img, label = img.float().to(device), label.float().to(device)\n",
    "\n",
    "        # Data -> Model -> Output\n",
    "        logit = model_test(img)\n",
    "        logit = torch.squeeze(logit)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = criterion(logit, label)\n",
    "\n",
    "        test_loss.append(loss.item())\n",
    "        f_pred.extend(logit.argmax(1).detach().cpu().numpy().tolist())\n",
    "        \n",
    "    print(m_path[len('/home/lab17/jupyter_home/saved_models/'):])\n",
    "\n",
    "    print('test loss :' ,np.mean(test_loss))\n",
    "    print('test acc :', score_function(test_y, f_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6275372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_pred = []\n",
    "# pred_prob = []\n",
    "\n",
    "# image_data = Image.open('home/lab17/jupyter_home/Data/product_test/img.jpg')\n",
    "\n",
    "# image_transform = transforms.Compose([\n",
    "#     transforms.Resize(size=256),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.744859, 0.735139, 0.711357],\n",
    "#                          std=[0.100712, 0.120692, 0.167998])\n",
    "# ])\n",
    "\n",
    "# x = image_transform(image_data)\n",
    "# pred = model_test(x)\n",
    "# pred_prob.extend(pred.detach().cpu().numpy())\n",
    "# f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())\n",
    "\n",
    "# label_decoder = {val:key for key, val in zip(range(CFG['class']), sorted(label_list))}\n",
    "\n",
    "# f_result = [label_decoder[result] for result in f_pred]\n",
    "\n",
    "# print(f_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_17] *",
   "language": "python",
   "name": "conda-env-pytorch_17-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
